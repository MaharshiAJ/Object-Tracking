{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MJ3128/CS-GY-6613-Assignments/blob/main/Assignment%203/Task_2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "tqD5YcYbUBHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the feature extraction model"
      ],
      "metadata": {
        "id": "r0v8RnNR5p7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Assignment 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auwnl6LRBRY7",
        "outputId": "cd061346-cb63-4546-d20f-9f328c7e7a37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/Assignment 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dCdZj3fZ4VVG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from PIL import Image\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
        "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
        "model = model.cuda()\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features = None"
      ],
      "metadata": {
        "id": "Niz2PHYZpjpx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_hook_fn(module, input, output):\n",
        "  global extracted_features\n",
        "  extracted_features = input[0]\n",
        "\n",
        "model._modules.get(\"roi_heads\")._modules.get(\"box_predictor\").register_forward_hook(box_hook_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrRRJiZh4FXG",
        "outputId": "5a71c91f-48fb-4f8b-c449-cc5aa5d86614"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7f73739f3370>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Deep Sort"
      ],
      "metadata": {
        "id": "fdLkZQya575E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import load_img, img_to_array\n",
        "from numpy import expand_dims\n",
        "import cv2\n",
        "import colorsys\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "import random\n",
        "\n",
        "label_map = {\n",
        "    \"person\": \"blue\",\n",
        "    \"sports ball\" : \"blue\"\n",
        "}\n",
        " \n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "        self.xmin = xmin\n",
        "        self.ymin = ymin\n",
        "        self.xmax = xmax\n",
        "        self.ymax = ymax\n",
        "        self.objness = objness\n",
        "        self.classes = classes\n",
        "        self.label = -1\n",
        "        self.score = -1\n",
        "\n",
        "    def get_label(self):\n",
        "        if self.label == -1:\n",
        "            self.label = np.argmax(self.classes)\n",
        "\n",
        "        return self.label\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.score == -1:\n",
        "            self.score = self.classes[self.get_label()]\n",
        " \n",
        "        return self.score\n",
        " \n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        " \n",
        "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
        "    grid_h, grid_w = netout.shape[:2]\n",
        "    nb_box = 3\n",
        "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "    nb_class = netout.shape[-1] - 5\n",
        "    boxes = []\n",
        "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        " \n",
        "    for i in range(grid_h*grid_w):\n",
        "        row = i / grid_w\n",
        "        col = i % grid_w\n",
        "        for b in range(nb_box):\n",
        "            # 4th element is objectness score\n",
        "            objectness = netout[int(row)][int(col)][b][4]\n",
        "            if(objectness.all() <= obj_thresh): continue\n",
        "            # first 4 elements are x, y, w, and h\n",
        "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
        "            x = (col + x) / grid_w # center position, unit: image width\n",
        "            y = (row + y) / grid_h # center position, unit: image height\n",
        "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
        "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
        "            # last elements are class probabilities\n",
        "            classes = netout[int(row)][col][b][5:]\n",
        "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "            boxes.append(box)\n",
        "    return boxes\n",
        " \n",
        "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
        "    new_w, new_h = net_w, net_h\n",
        "    for i in range(len(boxes)):\n",
        "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "    x1, x2 = interval_a\n",
        "    x3, x4 = interval_b\n",
        "    if x3 < x1:\n",
        "        if x4 < x1:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x1\n",
        "    else:\n",
        "        if x2 < x3:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x3\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "    intersect = intersect_w * intersect_h\n",
        "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "    union = w1*h1 + w2*h2 - intersect\n",
        "    return float(intersect) / union\n",
        " \n",
        "def do_nms(boxes, nms_thresh):\n",
        "    if len(boxes) > 0:\n",
        "        nb_class = len(boxes[0].classes)\n",
        "    else:\n",
        "        return\n",
        "    for c in range(nb_class):\n",
        "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "            if boxes[index_i].classes[c] == 0: continue\n",
        "            for j in range(i+1, len(sorted_indices)):\n",
        "                index_j = sorted_indices[j]\n",
        "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "                    boxes[index_j].classes[c] = 0\n",
        "\n",
        "# load and prepare an image\n",
        "def load_image_pixels(filename, shape):\n",
        "    # load the image to get its shape\n",
        "    image = load_img(filename)\n",
        "    width, height = image.size\n",
        "    # load the image with the required size\n",
        "    image = load_img(filename, target_size=shape)\n",
        "    # convert to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # scale pixel values to [0, 1]\n",
        "    image = image.astype('float32')\n",
        "    image /= 255.0\n",
        "    # add a dimension so that we have one sample\n",
        "    image = expand_dims(image, 0)\n",
        "    return image, width, height\n",
        "\n",
        "# Preprocess the given image\n",
        "def image_preprocess(image, target_size):\n",
        "    ih, iw    = target_size\n",
        "    h,  w, _  = image.shape\n",
        "\n",
        "    scale = min(iw/w, ih/h)\n",
        "    nw, nh  = int(scale * w), int(scale * h)\n",
        "    image_resized = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n",
        "    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n",
        "    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
        "    image_paded = image_paded / 255.\n",
        "\n",
        "    image_expanded = expand_dims(image_paded, 0)\n",
        "\n",
        "    return image_expanded, w, h\n",
        " \n",
        "# get all of the results above a threshold\n",
        "def get_boxes(boxes, labels, thresh, classes):\n",
        "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
        "    # enumerate all boxes\n",
        "    for box in boxes:\n",
        "        # enumerate all possible labels\n",
        "        for i in range(len(labels)):\n",
        "            # check if the threshold for this label is high enough\n",
        "            if box.classes[i] > thresh:\n",
        "                v_boxes.append(box)\n",
        "                v_labels.append(labels[i])\n",
        "                v_scores.append(box.classes[i]*100)\n",
        "                # don't break, many labels may trigger for one box\n",
        "    return v_boxes, v_labels, v_scores\n",
        " \n",
        "# draw all results\n",
        "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
        "    # load the image\n",
        "    data = pyplot.imread(filename)\n",
        "    # plot the image\n",
        "    pyplot.imshow(data)\n",
        "    # get the context for drawing boxes\n",
        "    ax = pyplot.gca()\n",
        "    # plot each box\n",
        "    for i in range(len(v_boxes)):\n",
        "        box = v_boxes[i]\n",
        "        # get coordinates\n",
        "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "        # calculate width and height of the box\n",
        "        width, height = x2 - x1, y2 - y1\n",
        "        # create the shape\n",
        "        rect = Rectangle((x1, y1), width, height, fill=False, color=label_map[v_labels[i]])\n",
        "        # draw the box\n",
        "        ax.add_patch(rect)\n",
        "        # draw text and score in top left corner\n",
        "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
        "        pyplot.text(x1, y1, label, color=label_map[v_labels[i]])\n",
        "    # show the plot\n",
        "    pyplot.show()\n",
        "    \n",
        "def draw_bbox(image, bboxes, CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
        "  NUM_CLASS = CLASSES\n",
        "  num_classes = len(NUM_CLASS)\n",
        "  image_h, image_w, _ = image.shape\n",
        "  hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "\n",
        "  colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "  colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  random.seed(None)\n",
        "\n",
        "  for i, bbox in enumerate(bboxes):\n",
        "      coor = np.array(bbox[:4], dtype=np.int32)\n",
        "      score = bbox[4]\n",
        "      class_ind = int(bbox[5])\n",
        "      \n",
        "      bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
        "      bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
        "      \n",
        "      if bbox_thick < 1: bbox_thick = 1\n",
        "      fontScale = 0.75 * bbox_thick\n",
        "      (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
        "\n",
        "      # put object rectangle\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, 2)\n",
        "\n",
        "      # if show_label:\n",
        "      #     # get text label\n",
        "      #     score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n",
        "\n",
        "      #     if tracking: score_str = \" \"+str(score)\n",
        "\n",
        "      #     label = \"{}\".format(NUM_CLASS[class_ind]) + score_str\n",
        "\n",
        "      #     # get text size\n",
        "      #     (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "      #                                                           fontScale, thickness=bbox_thick)\n",
        "      #     # put filled text rectangle\n",
        "      #     cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
        "\n",
        "      #     # put text above rectangle\n",
        "      #     cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "      #                 fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "or8PG698Dx-p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the probability threshold for detected objects\n",
        "class_threshold = 0.8"
      ],
      "metadata": {
        "id": "Q1dpfE6AEeG1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import errno\n",
        "import argparse\n",
        "import scipy.linalg\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class Detection(object):\n",
        "    \"\"\"\n",
        "    This class represents a bounding box detection in a single image.\n",
        "    Parameters\n",
        "    ----------\n",
        "    tlwh : array_like\n",
        "        Bounding box in format `(x, y, w, h)`.\n",
        "    confidence : float\n",
        "        Detector confidence score.\n",
        "    feature : array_like\n",
        "        A feature vector that describes the object contained in this image.\n",
        "    Attributes\n",
        "    ----------\n",
        "    tlwh : ndarray\n",
        "        Bounding box in format `(top left x, top left y, width, height)`.\n",
        "    confidence : ndarray\n",
        "        Detector confidence score.\n",
        "    class_name : ndarray\n",
        "        Detector class.\n",
        "    feature : ndarray | NoneType\n",
        "        A feature vector that describes the object contained in this image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tlwh, confidence, class_name, feature):\n",
        "        self.tlwh = np.asarray(tlwh, dtype=float)\n",
        "        self.confidence = float(confidence)\n",
        "        self.class_name = class_name\n",
        "        self.feature = np.asarray(feature, dtype=np.float32)\n",
        "\n",
        "    def get_class(self):\n",
        "        return self.class_name\n",
        "\n",
        "    def to_tlbr(self):\n",
        "        \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n",
        "        `(top left, bottom right)`.\n",
        "        \"\"\"\n",
        "        ret = self.tlwh.copy()\n",
        "        ret[2:] += ret[:2]\n",
        "        return ret\n",
        "\n",
        "    def to_xyah(self):\n",
        "        \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n",
        "        height)`, where the aspect ratio is `width / height`.\n",
        "        \"\"\"\n",
        "        ret = self.tlwh.copy()\n",
        "        ret[:2] += ret[2:] / 2\n",
        "        ret[2] /= ret[3]\n",
        "        return ret"
      ],
      "metadata": {
        "id": "v54LkfDvEfxM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Table for the 0.95 quantile of the chi-square distribution with N degrees of\n",
        "freedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\n",
        "function and used as Mahalanobis gating threshold.\n",
        "\"\"\"\n",
        "chi2inv95 = {\n",
        "    1: 3.8415,\n",
        "    2: 5.9915,\n",
        "    3: 7.8147,\n",
        "    4: 9.4877,\n",
        "    5: 11.070,\n",
        "    6: 12.592,\n",
        "    7: 14.067,\n",
        "    8: 15.507,\n",
        "    9: 16.919}\n",
        "\n",
        "\n",
        "class KalmanFilter(object):\n",
        "    \"\"\"\n",
        "    A simple Kalman filter for tracking bounding boxes in image space.\n",
        "    The 8-dimensional state space\n",
        "        x, y, a, h, vx, vy, va, vh\n",
        "    contains the bounding box center position (x, y), aspect ratio a, height h,\n",
        "    and their respective velocities.\n",
        "    Object motion follows a constant velocity model. The bounding box location\n",
        "    (x, y, a, h) is taken as direct observation of the state space (linear\n",
        "    observation model).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        ndim, dt = 4, 1.\n",
        "\n",
        "        # Create Kalman filter model matrices.\n",
        "        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n",
        "        for i in range(ndim):\n",
        "            self._motion_mat[i, ndim + i] = dt\n",
        "        self._update_mat = np.eye(ndim, 2 * ndim)\n",
        "\n",
        "        # Motion and observation uncertainty are chosen relative to the current\n",
        "        # state estimate. These weights control the amount of uncertainty in\n",
        "        # the model. This is a bit hacky.\n",
        "        self._std_weight_position = 1. / 20\n",
        "        self._std_weight_velocity = 1. / 160\n",
        "\n",
        "    def initiate(self, measurement):\n",
        "        \"\"\"Create track from unassociated measurement.\n",
        "        Parameters\n",
        "        ----------\n",
        "        measurement : ndarray\n",
        "            Bounding box coordinates (x, y, a, h) with center position (x, y),\n",
        "            aspect ratio a, and height h.\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n",
        "            dimensional) of the new track. Unobserved velocities are initialized\n",
        "            to 0 mean.\n",
        "        \"\"\"\n",
        "        mean_pos = measurement\n",
        "        mean_vel = np.zeros_like(mean_pos)\n",
        "        mean = np.r_[mean_pos, mean_vel]\n",
        "\n",
        "        std = [\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            1e-2,\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            10 * self._std_weight_velocity * measurement[3],\n",
        "            10 * self._std_weight_velocity * measurement[3],\n",
        "            1e-5,\n",
        "            10 * self._std_weight_velocity * measurement[3]]\n",
        "        covariance = np.diag(np.square(std))\n",
        "        return mean, covariance\n",
        "\n",
        "    def predict(self, mean, covariance):\n",
        "        \"\"\"Run Kalman filter prediction step.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The 8 dimensional mean vector of the object state at the previous\n",
        "            time step.\n",
        "        covariance : ndarray\n",
        "            The 8x8 dimensional covariance matrix of the object state at the\n",
        "            previous time step.\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the mean vector and covariance matrix of the predicted\n",
        "            state. Unobserved velocities are initialized to 0 mean.\n",
        "        \"\"\"\n",
        "        std_pos = [\n",
        "            self._std_weight_position * mean[3],\n",
        "            self._std_weight_position * mean[3],\n",
        "            1e-2,\n",
        "            self._std_weight_position * mean[3]]\n",
        "        std_vel = [\n",
        "            self._std_weight_velocity * mean[3],\n",
        "            self._std_weight_velocity * mean[3],\n",
        "            1e-5,\n",
        "            self._std_weight_velocity * mean[3]]\n",
        "        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n",
        "\n",
        "        mean = np.dot(self._motion_mat, mean)\n",
        "        covariance = np.linalg.multi_dot((\n",
        "            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n",
        "\n",
        "        return mean, covariance\n",
        "\n",
        "    def project(self, mean, covariance):\n",
        "        \"\"\"Project state distribution to measurement space.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The state's mean vector (8 dimensional array).\n",
        "        covariance : ndarray\n",
        "            The state's covariance matrix (8x8 dimensional).\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the projected mean and covariance matrix of the given state\n",
        "            estimate.\n",
        "        \"\"\"\n",
        "        std = [\n",
        "            self._std_weight_position * mean[3],\n",
        "            self._std_weight_position * mean[3],\n",
        "            1e-1,\n",
        "            self._std_weight_position * mean[3]]\n",
        "        innovation_cov = np.diag(np.square(std))\n",
        "\n",
        "        mean = np.dot(self._update_mat, mean)\n",
        "        covariance = np.linalg.multi_dot((\n",
        "            self._update_mat, covariance, self._update_mat.T))\n",
        "        return mean, covariance + innovation_cov\n",
        "\n",
        "    def update(self, mean, covariance, measurement):\n",
        "        \"\"\"Run Kalman filter correction step.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The predicted state's mean vector (8 dimensional).\n",
        "        covariance : ndarray\n",
        "            The state's covariance matrix (8x8 dimensional).\n",
        "        measurement : ndarray\n",
        "            The 4 dimensional measurement vector (x, y, a, h), where (x, y)\n",
        "            is the center position, a the aspect ratio, and h the height of the\n",
        "            bounding box.\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the measurement-corrected state distribution.\n",
        "        \"\"\"\n",
        "        projected_mean, projected_cov = self.project(mean, covariance)\n",
        "\n",
        "        chol_factor, lower = scipy.linalg.cho_factor(\n",
        "            projected_cov, lower=True, check_finite=False)\n",
        "        kalman_gain = scipy.linalg.cho_solve(\n",
        "            (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\n",
        "            check_finite=False).T\n",
        "        innovation = measurement - projected_mean\n",
        "\n",
        "        new_mean = mean + np.dot(innovation, kalman_gain.T)\n",
        "        new_covariance = covariance - np.linalg.multi_dot((\n",
        "            kalman_gain, projected_cov, kalman_gain.T))\n",
        "        return new_mean, new_covariance\n",
        "\n",
        "    def gating_distance(self, mean, covariance, measurements,\n",
        "                        only_position=False):\n",
        "        \"\"\"Compute gating distance between state distribution and measurements.\n",
        "        A suitable distance threshold can be obtained from `chi2inv95`. If\n",
        "        `only_position` is False, the chi-square distribution has 4 degrees of\n",
        "        freedom, otherwise 2.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            Mean vector over the state distribution (8 dimensional).\n",
        "        covariance : ndarray\n",
        "            Covariance of the state distribution (8x8 dimensional).\n",
        "        measurements : ndarray\n",
        "            An Nx4 dimensional matrix of N measurements, each in\n",
        "            format (x, y, a, h) where (x, y) is the bounding box center\n",
        "            position, a the aspect ratio, and h the height.\n",
        "        only_position : Optional[bool]\n",
        "            If True, distance computation is done with respect to the bounding\n",
        "            box center position only.\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            Returns an array of length N, where the i-th element contains the\n",
        "            squared Mahalanobis distance between (mean, covariance) and\n",
        "            `measurements[i]`.\n",
        "        \"\"\"\n",
        "        mean, covariance = self.project(mean, covariance)\n",
        "        if only_position:\n",
        "            mean, covariance = mean[:2], covariance[:2, :2]\n",
        "            measurements = measurements[:, :2]\n",
        "\n",
        "        cholesky_factor = np.linalg.cholesky(covariance)\n",
        "        d = measurements - mean\n",
        "        z = scipy.linalg.solve_triangular(\n",
        "            cholesky_factor, d.T, lower=True, check_finite=False,\n",
        "            overwrite_b=True)\n",
        "        squared_maha = np.sum(z * z, axis=0)\n",
        "        return squared_maha\n"
      ],
      "metadata": {
        "id": "femwXZnkE8m3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INFTY_COST = 1e+5\n",
        "\n",
        "\n",
        "def min_cost_matching(\n",
        "        distance_metric, max_distance, tracks, detections, track_indices=None,\n",
        "        detection_indices=None):\n",
        "    \"\"\"Solve linear assignment problem.\n",
        "    Parameters\n",
        "    ----------\n",
        "    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n",
        "        The distance metric is given a list of tracks and detections as well as\n",
        "        a list of N track indices and M detection indices. The metric should\n",
        "        return the NxM dimensional cost matrix, where element (i, j) is the\n",
        "        association cost between the i-th track in the given track indices and\n",
        "        the j-th detection in the given detection_indices.\n",
        "    max_distance : float\n",
        "        Gating threshold. Associations with cost larger than this value are\n",
        "        disregarded.\n",
        "    tracks : List[track.Track]\n",
        "        A list of predicted tracks at the current time step.\n",
        "    detections : List[detection.Detection]\n",
        "        A list of detections at the current time step.\n",
        "    track_indices : List[int]\n",
        "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
        "        `tracks` (see description above).\n",
        "    detection_indices : List[int]\n",
        "        List of detection indices that maps columns in `cost_matrix` to\n",
        "        detections in `detections` (see description above).\n",
        "    Returns\n",
        "    -------\n",
        "    (List[(int, int)], List[int], List[int])\n",
        "        Returns a tuple with the following three entries:\n",
        "        * A list of matched track and detection indices.\n",
        "        * A list of unmatched track indices.\n",
        "        * A list of unmatched detection indices.\n",
        "    \"\"\"\n",
        "    if track_indices is None:\n",
        "        track_indices = np.arange(len(tracks))\n",
        "    if detection_indices is None:\n",
        "        detection_indices = np.arange(len(detections))\n",
        "\n",
        "    if len(detection_indices) == 0 or len(track_indices) == 0:\n",
        "        return [], track_indices, detection_indices  # Nothing to match.\n",
        "\n",
        "    cost_matrix = distance_metric(\n",
        "        tracks, detections, track_indices, detection_indices)\n",
        "    cost_matrix[cost_matrix > max_distance] = max_distance + 1e-5\n",
        "    indices = linear_sum_assignment(cost_matrix)\n",
        "    indices = np.asarray(indices)\n",
        "    indices = np.transpose(indices)\n",
        "    matches, unmatched_tracks, unmatched_detections = [], [], []\n",
        "    for col, detection_idx in enumerate(detection_indices):\n",
        "        if col not in indices[:, 1]:\n",
        "            unmatched_detections.append(detection_idx)\n",
        "    for row, track_idx in enumerate(track_indices):\n",
        "        if row not in indices[:, 0]:\n",
        "            unmatched_tracks.append(track_idx)\n",
        "    for row, col in indices:\n",
        "        track_idx = track_indices[row]\n",
        "        detection_idx = detection_indices[col]\n",
        "        if cost_matrix[row, col] > max_distance:\n",
        "            unmatched_tracks.append(track_idx)\n",
        "            unmatched_detections.append(detection_idx)\n",
        "        else:\n",
        "            matches.append((track_idx, detection_idx))\n",
        "    return matches, unmatched_tracks, unmatched_detections\n",
        "\n",
        "\n",
        "def matching_cascade(\n",
        "        distance_metric, max_distance, cascade_depth, tracks, detections,\n",
        "        track_indices=None, detection_indices=None):\n",
        "    \"\"\"Run matching cascade.\n",
        "    Parameters\n",
        "    ----------\n",
        "    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n",
        "        The distance metric is given a list of tracks and detections as well as\n",
        "        a list of N track indices and M detection indices. The metric should\n",
        "        return the NxM dimensional cost matrix, where element (i, j) is the\n",
        "        association cost between the i-th track in the given track indices and\n",
        "        the j-th detection in the given detection indices.\n",
        "    max_distance : float\n",
        "        Gating threshold. Associations with cost larger than this value are\n",
        "        disregarded.\n",
        "    cascade_depth: int\n",
        "        The cascade depth, should be se to the maximum track age.\n",
        "    tracks : List[track.Track]\n",
        "        A list of predicted tracks at the current time step.\n",
        "    detections : List[detection.Detection]\n",
        "        A list of detections at the current time step.\n",
        "    track_indices : Optional[List[int]]\n",
        "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
        "        `tracks` (see description above). Defaults to all tracks.\n",
        "    detection_indices : Optional[List[int]]\n",
        "        List of detection indices that maps columns in `cost_matrix` to\n",
        "        detections in `detections` (see description above). Defaults to all\n",
        "        detections.\n",
        "    Returns\n",
        "    -------\n",
        "    (List[(int, int)], List[int], List[int])\n",
        "        Returns a tuple with the following three entries:\n",
        "        * A list of matched track and detection indices.\n",
        "        * A list of unmatched track indices.\n",
        "        * A list of unmatched detection indices.\n",
        "    \"\"\"\n",
        "    if track_indices is None:\n",
        "        track_indices = list(range(len(tracks)))\n",
        "    if detection_indices is None:\n",
        "        detection_indices = list(range(len(detections)))\n",
        "\n",
        "    unmatched_detections = detection_indices\n",
        "    matches = []\n",
        "    for level in range(cascade_depth):\n",
        "        if len(unmatched_detections) == 0:  # No detections left\n",
        "            break\n",
        "\n",
        "        track_indices_l = [\n",
        "            k for k in track_indices\n",
        "            if tracks[k].time_since_update == 1 + level\n",
        "        ]\n",
        "        if len(track_indices_l) == 0:  # Nothing to match at this level\n",
        "            continue\n",
        "\n",
        "        matches_l, _, unmatched_detections = \\\n",
        "            min_cost_matching(\n",
        "                distance_metric, max_distance, tracks, detections,\n",
        "                track_indices_l, unmatched_detections)\n",
        "        matches += matches_l\n",
        "    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))\n",
        "    return matches, unmatched_tracks, unmatched_detections\n",
        "\n",
        "\n",
        "def gate_cost_matrix(\n",
        "        kf, cost_matrix, tracks, detections, track_indices, detection_indices,\n",
        "        gated_cost=INFTY_COST, only_position=False):\n",
        "    \"\"\"Invalidate infeasible entries in cost matrix based on the state\n",
        "    distributions obtained by Kalman filtering.\n",
        "    Parameters\n",
        "    ----------\n",
        "    kf : The Kalman filter.\n",
        "    cost_matrix : ndarray\n",
        "        The NxM dimensional cost matrix, where N is the number of track indices\n",
        "        and M is the number of detection indices, such that entry (i, j) is the\n",
        "        association cost between `tracks[track_indices[i]]` and\n",
        "        `detections[detection_indices[j]]`.\n",
        "    tracks : List[track.Track]\n",
        "        A list of predicted tracks at the current time step.\n",
        "    detections : List[detection.Detection]\n",
        "        A list of detections at the current time step.\n",
        "    track_indices : List[int]\n",
        "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
        "        `tracks` (see description above).\n",
        "    detection_indices : List[int]\n",
        "        List of detection indices that maps columns in `cost_matrix` to\n",
        "        detections in `detections` (see description above).\n",
        "    gated_cost : Optional[float]\n",
        "        Entries in the cost matrix corresponding to infeasible associations are\n",
        "        set this value. Defaults to a very large value.\n",
        "    only_position : Optional[bool]\n",
        "        If True, only the x, y position of the state distribution is considered\n",
        "        during gating. Defaults to False.\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        Returns the modified cost matrix.\n",
        "    \"\"\"\n",
        "    gating_dim = 2 if only_position else 4\n",
        "    gating_threshold = chi2inv95[gating_dim]\n",
        "    measurements = np.asarray(\n",
        "        [detections[i].to_xyah() for i in detection_indices])\n",
        "    for row, track_idx in enumerate(track_indices):\n",
        "        track = tracks[track_idx]\n",
        "        gating_distance = kf.gating_distance(\n",
        "            track.mean, track.covariance, measurements, only_position)\n",
        "        cost_matrix[row, gating_distance > gating_threshold] = gated_cost\n",
        "    return cost_matrix"
      ],
      "metadata": {
        "id": "klWB_HwvFA08"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(bbox, candidates):\n",
        "    \"\"\"Computer intersection over union.\n",
        "    Parameters\n",
        "    ----------\n",
        "    bbox : ndarray\n",
        "        A bounding box in format `(top left x, top left y, width, height)`.\n",
        "    candidates : ndarray\n",
        "        A matrix of candidate bounding boxes (one per row) in the same format\n",
        "        as `bbox`.\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        The intersection over union in [0, 1] between the `bbox` and each\n",
        "        candidate. A higher score means a larger fraction of the `bbox` is\n",
        "        occluded by the candidate.\n",
        "    \"\"\"\n",
        "    bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\n",
        "    candidates_tl = candidates[:, :2]\n",
        "    candidates_br = candidates[:, :2] + candidates[:, 2:]\n",
        "\n",
        "    tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\n",
        "               np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\n",
        "    br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\n",
        "               np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\n",
        "    wh = np.maximum(0., br - tl)\n",
        "\n",
        "    area_intersection = wh.prod(axis=1)\n",
        "    area_bbox = bbox[2:].prod()\n",
        "    area_candidates = candidates[:, 2:].prod(axis=1)\n",
        "    return area_intersection / (area_bbox + area_candidates - area_intersection)\n",
        "\n",
        "\n",
        "def iou_cost(tracks, detections, track_indices=None,\n",
        "             detection_indices=None):\n",
        "    \"\"\"An intersection over union distance metric.\n",
        "    Parameters\n",
        "    ----------\n",
        "    tracks : List[deep_sort.track.Track]\n",
        "        A list of tracks.\n",
        "    detections : List[deep_sort.detection.Detection]\n",
        "        A list of detections.\n",
        "    track_indices : Optional[List[int]]\n",
        "        A list of indices to tracks that should be matched. Defaults to\n",
        "        all `tracks`.\n",
        "    detection_indices : Optional[List[int]]\n",
        "        A list of indices to detections that should be matched. Defaults\n",
        "        to all `detections`.\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        Returns a cost matrix of shape\n",
        "        len(track_indices), len(detection_indices) where entry (i, j) is\n",
        "        `1 - iou(tracks[track_indices[i]], detections[detection_indices[j]])`.\n",
        "    \"\"\"\n",
        "    if track_indices is None:\n",
        "        track_indices = np.arange(len(tracks))\n",
        "    if detection_indices is None:\n",
        "        detection_indices = np.arange(len(detections))\n",
        "\n",
        "    cost_matrix = np.zeros((len(track_indices), len(detection_indices)))\n",
        "    for row, track_idx in enumerate(track_indices):\n",
        "        if tracks[track_idx].time_since_update > 1:\n",
        "            cost_matrix[row, :] = linear_sum_assignment.INFTY_COST\n",
        "            continue\n",
        "\n",
        "        bbox = tracks[track_idx].to_tlwh()\n",
        "        candidates = np.asarray([detections[i].tlwh for i in detection_indices])\n",
        "        cost_matrix[row, :] = 1. - iou(bbox, candidates)\n",
        "    return cost_matrix"
      ],
      "metadata": {
        "id": "0gJS9QxbFCnL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression(boxes, classes, max_bbox_overlap, scores=None):\n",
        "    \"\"\"Suppress overlapping detections.\n",
        "    Original code from [1]_ has been adapted to include confidence score.\n",
        "    .. [1] http://www.pyimagesearch.com/2015/02/16/\n",
        "           faster-non-maximum-suppression-python/\n",
        "    Examples\n",
        "    --------\n",
        "        >>> boxes = [d.roi for d in detections]\n",
        "        >>> classes = [d.classes for d in detections]\n",
        "        >>> scores = [d.confidence for d in detections]\n",
        "        >>> indices = non_max_suppression(boxes, max_bbox_overlap, scores)\n",
        "        >>> detections = [detections[i] for i in indices]\n",
        "    Parameters\n",
        "    ----------\n",
        "    boxes : ndarray\n",
        "        Array of ROIs (x, y, width, height).\n",
        "    max_bbox_overlap : float\n",
        "        ROIs that overlap more than this values are suppressed.\n",
        "    scores : Optional[array_like]\n",
        "        Detector confidence score.\n",
        "    Returns\n",
        "    -------\n",
        "    List[int]\n",
        "        Returns indices of detections that have survived non-maxima suppression.\n",
        "    \"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    boxes = boxes.cpu().detach().numpy()\n",
        "    pick = []\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2] + boxes[:, 0]\n",
        "    y2 = boxes[:, 3] + boxes[:, 1]\n",
        "\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    if scores is not None:\n",
        "        idxs = np.argsort(scores)\n",
        "    else:\n",
        "        idxs = np.argsort(y2)\n",
        "\n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "\n",
        "        idxs = np.delete(\n",
        "            idxs, np.concatenate(\n",
        "                ([last], np.where(overlap > max_bbox_overlap)[0])))\n",
        "\n",
        "    return pick"
      ],
      "metadata": {
        "id": "_0UbzyNKFFis"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _pdist(a, b):\n",
        "    \"\"\"Compute pair-wise squared distance between points in `a` and `b`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : array_like\n",
        "        An NxM matrix of N samples of dimensionality M.\n",
        "    b : array_like\n",
        "        An LxM matrix of L samples of dimensionality M.\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n",
        "        contains the squared distance between `a[i]` and `b[j]`.\n",
        "    \"\"\"\n",
        "    a, b = np.asarray(a), np.asarray(b)\n",
        "    if len(a) == 0 or len(b) == 0:\n",
        "        return np.zeros((len(a), len(b)))\n",
        "    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n",
        "    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n",
        "    r2 = np.clip(r2, 0., float(np.inf))\n",
        "    return r2\n",
        "\n",
        "\n",
        "def _cosine_distance(a, b, data_is_normalized=False):\n",
        "    \"\"\"Compute pair-wise cosine distance between points in `a` and `b`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : array_like\n",
        "        An NxM matrix of N samples of dimensionality M.\n",
        "    b : array_like\n",
        "        An LxM matrix of L samples of dimensionality M.\n",
        "    data_is_normalized : Optional[bool]\n",
        "        If True, assumes rows in a and b are unit length vectors.\n",
        "        Otherwise, a and b are explicitly normalized to lenght 1.\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n",
        "        contains the squared distance between `a[i]` and `b[j]`.\n",
        "    \"\"\"\n",
        "    if not data_is_normalized:\n",
        "        a = np.asarray(a) / np.linalg.norm(a, axis=1, keepdims=True)\n",
        "        b = np.asarray(b) / np.linalg.norm(b, axis=1, keepdims=True)\n",
        "    return 1. - np.dot(a, b.T)\n",
        "\n",
        "\n",
        "def _nn_euclidean_distance(x, y):\n",
        "    \"\"\" Helper function for nearest neighbor distance metric (Euclidean).\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        A matrix of N row-vectors (sample points).\n",
        "    y : ndarray\n",
        "        A matrix of M row-vectors (query points).\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        A vector of length M that contains for each entry in `y` the\n",
        "        smallest Euclidean distance to a sample in `x`.\n",
        "    \"\"\"\n",
        "    distances = _pdist(x, y)\n",
        "    return np.maximum(0.0, distances.min(axis=0))\n",
        "\n",
        "\n",
        "def _nn_cosine_distance(x, y):\n",
        "    \"\"\" Helper function for nearest neighbor distance metric (cosine).\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "        A matrix of N row-vectors (sample points).\n",
        "    y : ndarray\n",
        "        A matrix of M row-vectors (query points).\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "        A vector of length M that contains for each entry in `y` the\n",
        "        smallest cosine distance to a sample in `x`.\n",
        "    \"\"\"\n",
        "    distances = _cosine_distance(x, y)\n",
        "    return distances.min(axis=0)\n",
        "\n",
        "\n",
        "class NearestNeighborDistanceMetric(object):\n",
        "    \"\"\"\n",
        "    A nearest neighbor distance metric that, for each target, returns\n",
        "    the closest distance to any sample that has been observed so far.\n",
        "    Parameters\n",
        "    ----------\n",
        "    metric : str\n",
        "        Either \"euclidean\" or \"cosine\".\n",
        "    matching_threshold: float\n",
        "        The matching threshold. Samples with larger distance are considered an\n",
        "        invalid match.\n",
        "    budget : Optional[int]\n",
        "        If not None, fix samples per class to at most this number. Removes\n",
        "        the oldest samples when the budget is reached.\n",
        "    Attributes\n",
        "    ----------\n",
        "    samples : Dict[int -> List[ndarray]]\n",
        "        A dictionary that maps from target identities to the list of samples\n",
        "        that have been observed so far.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metric, matching_threshold, budget=None):\n",
        "\n",
        "\n",
        "        if metric == \"euclidean\":\n",
        "            self._metric = _nn_euclidean_distance\n",
        "        elif metric == \"cosine\":\n",
        "            self._metric = _nn_cosine_distance\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Invalid metric; must be either 'euclidean' or 'cosine'\")\n",
        "        self.matching_threshold = matching_threshold\n",
        "        self.budget = budget\n",
        "        self.samples = {}\n",
        "\n",
        "    def partial_fit(self, features, targets, active_targets):\n",
        "        \"\"\"Update the distance metric with new data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        features : ndarray\n",
        "            An NxM matrix of N features of dimensionality M.\n",
        "        targets : ndarray\n",
        "            An integer array of associated target identities.\n",
        "        active_targets : List[int]\n",
        "            A list of targets that are currently present in the scene.\n",
        "        \"\"\"\n",
        "        for feature, target in zip(features, targets):\n",
        "            self.samples.setdefault(target, []).append(feature)\n",
        "            if self.budget is not None:\n",
        "                self.samples[target] = self.samples[target][-self.budget:]\n",
        "        self.samples = {k: self.samples[k] for k in active_targets}\n",
        "\n",
        "    def distance(self, features, targets):\n",
        "        \"\"\"Compute distance between features and targets.\n",
        "        Parameters\n",
        "        ----------\n",
        "        features : ndarray\n",
        "            An NxM matrix of N features of dimensionality M.\n",
        "        targets : List[int]\n",
        "            A list of targets to match the given `features` against.\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            Returns a cost matrix of shape len(targets), len(features), where\n",
        "            element (i, j) contains the closest squared distance between\n",
        "            `targets[i]` and `features[j]`.\n",
        "        \"\"\"\n",
        "        cost_matrix = np.zeros((len(targets), len(features)))\n",
        "        for i, target in enumerate(targets):\n",
        "            cost_matrix[i, :] = self._metric(self.samples[target], features)\n",
        "        return cost_matrix"
      ],
      "metadata": {
        "id": "hQ0vTRamFH_W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrackState:\n",
        "    \"\"\"\n",
        "    Enumeration type for the single target track state. Newly created tracks are\n",
        "    classified as `tentative` until enough evidence has been collected. Then,\n",
        "    the track state is changed to `confirmed`. Tracks that are no longer alive\n",
        "    are classified as `deleted` to mark them for removal from the set of active\n",
        "    tracks.\n",
        "    \"\"\"\n",
        "\n",
        "    Tentative = 1\n",
        "    Confirmed = 2\n",
        "    Deleted = 3\n",
        "\n",
        "\n",
        "class Track:\n",
        "    \"\"\"\n",
        "    A single target track with state space `(x, y, a, h)` and associated\n",
        "    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n",
        "    aspect ratio and `h` is the height.\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "        Mean vector of the initial state distribution.\n",
        "    covariance : ndarray\n",
        "        Covariance matrix of the initial state distribution.\n",
        "    track_id : int\n",
        "        A unique track identifier.\n",
        "    n_init : int\n",
        "        Number of consecutive detections before the track is confirmed. The\n",
        "        track state is set to `Deleted` if a miss occurs within the first\n",
        "        `n_init` frames.\n",
        "    max_age : int\n",
        "        The maximum number of consecutive misses before the track state is\n",
        "        set to `Deleted`.\n",
        "    feature : Optional[ndarray]\n",
        "        Feature vector of the detection this track originates from. If not None,\n",
        "        this feature is added to the `features` cache.\n",
        "    Attributes\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "        Mean vector of the initial state distribution.\n",
        "    covariance : ndarray\n",
        "        Covariance matrix of the initial state distribution.\n",
        "    track_id : int\n",
        "        A unique track identifier.\n",
        "    hits : int\n",
        "        Total number of measurement updates.\n",
        "    age : int\n",
        "        Total number of frames since first occurance.\n",
        "    time_since_update : int\n",
        "        Total number of frames since last measurement update.\n",
        "    state : TrackState\n",
        "        The current track state.\n",
        "    features : List[ndarray]\n",
        "        A cache of features. On each measurement update, the associated feature\n",
        "        vector is added to this list.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, covariance, track_id, n_init, max_age,\n",
        "                 feature=None, class_name=None):\n",
        "        self.mean = mean\n",
        "        self.covariance = covariance\n",
        "        self.track_id = track_id\n",
        "        self.hits = 1\n",
        "        self.age = 1\n",
        "        self.time_since_update = 0\n",
        "\n",
        "        self.state = TrackState.Tentative\n",
        "        self.features = []\n",
        "        if feature is not None:\n",
        "            self.features.append(feature)\n",
        "\n",
        "        self._n_init = n_init\n",
        "        self._max_age = max_age\n",
        "        self.class_name = class_name\n",
        "\n",
        "    def to_tlwh(self):\n",
        "        \"\"\"Get current position in bounding box format `(top left x, top left y,\n",
        "        width, height)`.\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            The bounding box.\n",
        "        \"\"\"\n",
        "        ret = self.mean[:4].copy()\n",
        "        ret[2] *= ret[3]\n",
        "        ret[:2] -= ret[2:] / 2\n",
        "        return ret\n",
        "\n",
        "    def to_tlbr(self):\n",
        "        \"\"\"Get current position in bounding box format `(min x, miny, max x,\n",
        "        max y)`.\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            The bounding box.\n",
        "        \"\"\"\n",
        "        ret = self.to_tlwh()\n",
        "        ret[2:] = ret[:2] + ret[2:]\n",
        "        return ret\n",
        "    \n",
        "    def get_class(self):\n",
        "        return self.class_name\n",
        "\n",
        "    def predict(self, kf):\n",
        "        \"\"\"Propagate the state distribution to the current time step using a\n",
        "        Kalman filter prediction step.\n",
        "        Parameters\n",
        "        ----------\n",
        "        kf : kalman_filter.KalmanFilter\n",
        "            The Kalman filter.\n",
        "        \"\"\"\n",
        "        self.mean, self.covariance = kf.predict(self.mean, self.covariance)\n",
        "        self.age += 1\n",
        "        self.time_since_update += 1\n",
        "\n",
        "    def update(self, kf, detection):\n",
        "        \"\"\"Perform Kalman filter measurement update step and update the feature\n",
        "        cache.\n",
        "        Parameters\n",
        "        ----------\n",
        "        kf : kalman_filter.KalmanFilter\n",
        "            The Kalman filter.\n",
        "        detection : Detection\n",
        "            The associated detection.\n",
        "        \"\"\"\n",
        "        self.mean, self.covariance = kf.update(\n",
        "            self.mean, self.covariance, detection.to_xyah())\n",
        "        self.features.append(detection.feature)\n",
        "\n",
        "        self.hits += 1\n",
        "        self.time_since_update = 0\n",
        "        if self.state == TrackState.Tentative and self.hits >= self._n_init:\n",
        "            self.state = TrackState.Confirmed\n",
        "\n",
        "    def mark_missed(self):\n",
        "        \"\"\"Mark this track as missed (no association at the current time step).\n",
        "        \"\"\"\n",
        "        if self.state == TrackState.Tentative:\n",
        "            self.state = TrackState.Deleted\n",
        "        elif self.time_since_update > self._max_age:\n",
        "            self.state = TrackState.Deleted\n",
        "\n",
        "    def is_tentative(self):\n",
        "        \"\"\"Returns True if this track is tentative (unconfirmed).\n",
        "        \"\"\"\n",
        "        return self.state == TrackState.Tentative\n",
        "\n",
        "    def is_confirmed(self):\n",
        "        \"\"\"Returns True if this track is confirmed.\"\"\"\n",
        "        return self.state == TrackState.Confirmed\n",
        "\n",
        "    def is_deleted(self):\n",
        "        \"\"\"Returns True if this track is dead and should be deleted.\"\"\"\n",
        "        return self.state == TrackState.Deleted"
      ],
      "metadata": {
        "id": "h_BAGYIHFIoI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tracker:\n",
        "    \"\"\"\n",
        "    This is the multi-target tracker.\n",
        "    Parameters\n",
        "    ----------\n",
        "    metric : nn_matching.NearestNeighborDistanceMetric\n",
        "        A distance metric for measurement-to-track association.\n",
        "    max_age : int\n",
        "        Maximum number of missed misses before a track is deleted.\n",
        "    n_init : int\n",
        "        Number of consecutive detections before the track is confirmed. The\n",
        "        track state is set to `Deleted` if a miss occurs within the first\n",
        "        `n_init` frames.\n",
        "    Attributes\n",
        "    ----------\n",
        "    metric : nn_matching.NearestNeighborDistanceMetric\n",
        "        The distance metric used for measurement to track association.\n",
        "    max_age : int\n",
        "        Maximum number of missed misses before a track is deleted.\n",
        "    n_init : int\n",
        "        Number of frames that a track remains in initialization phase.\n",
        "    kf : kalman_filter.KalmanFilter\n",
        "        A Kalman filter to filter target trajectories in image space.\n",
        "    tracks : List[Track]\n",
        "        The list of active tracks at the current time step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metric, max_iou_distance=0.7, max_age=30, n_init=3):\n",
        "        self.metric = metric\n",
        "        self.max_iou_distance = max_iou_distance\n",
        "        self.max_age = max_age\n",
        "        self.n_init = n_init\n",
        "\n",
        "        self.kf = KalmanFilter()\n",
        "        self.tracks = []\n",
        "        self._next_id = 1\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"Propagate track state distributions one time step forward.\n",
        "        This function should be called once every time step, before `update`.\n",
        "        \"\"\"\n",
        "        for track in self.tracks:\n",
        "            track.predict(self.kf)\n",
        "\n",
        "    def update(self, detections):\n",
        "        \"\"\"Perform measurement update and track management.\n",
        "        Parameters\n",
        "        ----------\n",
        "        detections : List[deep_sort.detection.Detection]\n",
        "            A list of detections at the current time step.\n",
        "        \"\"\"\n",
        "        # Run matching cascade.\n",
        "        matches, unmatched_tracks, unmatched_detections = \\\n",
        "            self._match(detections)\n",
        "\n",
        "        # Update track set.\n",
        "        for track_idx, detection_idx in matches:\n",
        "            self.tracks[track_idx].update(\n",
        "                self.kf, detections[detection_idx])\n",
        "        for track_idx in unmatched_tracks:\n",
        "            self.tracks[track_idx].mark_missed()\n",
        "        for detection_idx in unmatched_detections:\n",
        "            self._initiate_track(detections[detection_idx])\n",
        "        self.tracks = [t for t in self.tracks if not t.is_deleted()]\n",
        "\n",
        "        # Update distance metric.\n",
        "        active_targets = [t.track_id for t in self.tracks if t.is_confirmed()]\n",
        "        features, targets = [], []\n",
        "        for track in self.tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            features += track.features\n",
        "            targets += [track.track_id for _ in track.features]\n",
        "            track.features = []\n",
        "        self.metric.partial_fit(\n",
        "            np.asarray(features), np.asarray(targets), active_targets)\n",
        "\n",
        "    def _match(self, detections):\n",
        "\n",
        "        def gated_metric(tracks, dets, track_indices, detection_indices):\n",
        "            features = np.array([dets[i].feature for i in detection_indices])\n",
        "            targets = np.array([tracks[i].track_id for i in track_indices])\n",
        "            cost_matrix = self.metric.distance(features, targets)\n",
        "            cost_matrix = gate_cost_matrix(\n",
        "                self.kf, cost_matrix, tracks, dets, track_indices,\n",
        "                detection_indices)\n",
        "\n",
        "            return cost_matrix\n",
        "\n",
        "        # Split track set into confirmed and unconfirmed tracks.\n",
        "        confirmed_tracks = [\n",
        "            i for i, t in enumerate(self.tracks) if t.is_confirmed()]\n",
        "        unconfirmed_tracks = [\n",
        "            i for i, t in enumerate(self.tracks) if not t.is_confirmed()]\n",
        "\n",
        "        # Associate confirmed tracks using appearance features.\n",
        "        matches_a, unmatched_tracks_a, unmatched_detections = \\\n",
        "                matching_cascade(\n",
        "                gated_metric, self.metric.matching_threshold, self.max_age,\n",
        "                self.tracks, detections, confirmed_tracks)\n",
        "\n",
        "        # Associate remaining tracks together with unconfirmed tracks using IOU.\n",
        "        iou_track_candidates = unconfirmed_tracks + [\n",
        "            k for k in unmatched_tracks_a if\n",
        "            self.tracks[k].time_since_update == 1]\n",
        "        unmatched_tracks_a = [\n",
        "            k for k in unmatched_tracks_a if\n",
        "            self.tracks[k].time_since_update != 1]\n",
        "        matches_b, unmatched_tracks_b, unmatched_detections = \\\n",
        "                min_cost_matching(\n",
        "                iou_cost, self.max_iou_distance, self.tracks,\n",
        "                detections, iou_track_candidates, unmatched_detections)\n",
        "\n",
        "        matches = matches_a + matches_b\n",
        "        unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b))\n",
        "        return matches, unmatched_tracks, unmatched_detections\n",
        "\n",
        "    def _initiate_track(self, detection):\n",
        "        mean, covariance = self.kf.initiate(detection.to_xyah())\n",
        "        class_name = detection.get_class()\n",
        "        self.tracks.append(Track(\n",
        "            mean, covariance, self._next_id, self.n_init, self.max_age,\n",
        "            detection.feature, class_name))\n",
        "        self._next_id += 1"
      ],
      "metadata": {
        "id": "ozoOSawmFKls"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Algorithm"
      ],
      "metadata": {
        "id": "u8BT9CmSA_do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COCO_CLASSES = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
      ],
      "metadata": {
        "id": "PL0k3WBD3YPl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the parameters\n",
        "max_cosine_distance = 0.7\n",
        "max_euclidean_distance = 0.7\n",
        "nn_budget = None\n",
        "\n",
        "key_list = []\n",
        "val_list = COCO_CLASSES\n",
        "\n",
        "for i in range(len(COCO_CLASSES)):\n",
        "  key_list.append(i)\n"
      ],
      "metadata": {
        "id": "lRwhotgLFMSw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Video\n",
        "frames, _, _ = torchvision.io.read_video(\"./Video/Football match.mp4\", \n",
        "                                pts_unit=\"sec\",\n",
        "                                output_format=\"TCHW\")\n",
        "frames = frames.cuda()"
      ],
      "metadata": {
        "id": "aCzaYeEm6TWi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def object_tracking(f, output_path = \"./output.mp4\"):\n",
        "  times, times_2 = [], []\n",
        "  metric = NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "  tracker = Tracker(metric)\n",
        "\n",
        "  preprocess = weights.transforms()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  codec = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "  out = cv2.VideoWriter(output_path, codec, 25, (1280, 720))\n",
        "\n",
        "  run_number = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for frame in f:\n",
        "      img = torchvision.transforms.ToPILImage()(frame)\n",
        "      img = np.asarray(img)\n",
        "      # img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "      t1 = time.time()\n",
        "      processed = preprocess(frame)\n",
        "      processed = processed.unsqueeze_(1)\n",
        "      predictions = model(processed)[0]\n",
        "      t2 = time.time()\n",
        "\n",
        "      inds = non_max_suppression(predictions[\"boxes\"], [\"person\", \"sports ball\"], 0.3)\n",
        "      boxes = predictions[\"boxes\"][inds].cpu().detach().numpy()\n",
        "      labels = predictions[\"labels\"][inds]\n",
        "      names = []\n",
        "      for label in labels.cpu().detach().numpy():\n",
        "        names.append(COCO_CLASSES[label])\n",
        "      scores = predictions[\"scores\"][inds].cpu().detach().numpy()\n",
        "      features = extracted_features[inds].cpu().detach().numpy()\n",
        "\n",
        "      detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(boxes, scores, names, features)]\n",
        "\n",
        "      tracker.predict()\n",
        "      tracker.update(detections)\n",
        "\n",
        "      tracked_bboxes = []\n",
        "\n",
        "      for track in tracker.tracks:\n",
        "        if track.time_since_update > 5:\n",
        "          continue\n",
        "        bbox = track.to_tlbr()\n",
        "        class_name = track.get_class() #Get the class name of particular object\n",
        "        tracking_id = track.track_id # Get the ID for the particular track\n",
        "        index = key_list[val_list.index(class_name)] # Get predicted object index by object name\n",
        "        tracked_bboxes.append(bbox.tolist() + [tracking_id, index]) # Structure data, that we could use it with our draw_bbox function\n",
        "\n",
        "      # image = draw_bbox(img.astype(np.int32), tracked_bboxes, CLASSES=COCO_CLASSES, tracking=True)\n",
        "      for i, bbox in enumerate(tracked_bboxes):\n",
        "        # put object rectangle\n",
        "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
        "\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      t3 = time.time()\n",
        "\n",
        "      times.append(t2-t1)\n",
        "      times_2.append(t3-t1)\n",
        "\n",
        "      times = times[-20:]\n",
        "      times_2 = times_2[-20:]\n",
        "\n",
        "      ms = sum(times)/len(times)*1000\n",
        "      fps = 1000 / ms\n",
        "      fps2 = 1000 / (sum(times_2)/len(times_2)*1000)\n",
        "\n",
        "      image = cv2.putText(img, \"Time: {:.1f} FPS\".format(fps), (0, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n",
        "      out.write(image)\n",
        "\n",
        "      run_number += 1\n",
        "      print(\"Frames Processed: \", run_number)\n",
        "\n",
        "    out.release()"
      ],
      "metadata": {
        "id": "7wYrR_9BBGRG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_tracking(frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGaonBOrcslx",
        "outputId": "e943c061-06de-4782-b67b-a7664747c43c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames Processed:  1\n",
            "Frames Processed:  2\n",
            "Frames Processed:  3\n",
            "Frames Processed:  4\n",
            "Frames Processed:  5\n",
            "Frames Processed:  6\n",
            "Frames Processed:  7\n",
            "Frames Processed:  8\n",
            "Frames Processed:  9\n",
            "Frames Processed:  10\n",
            "Frames Processed:  11\n",
            "Frames Processed:  12\n",
            "Frames Processed:  13\n",
            "Frames Processed:  14\n",
            "Frames Processed:  15\n",
            "Frames Processed:  16\n",
            "Frames Processed:  17\n",
            "Frames Processed:  18\n",
            "Frames Processed:  19\n",
            "Frames Processed:  20\n",
            "Frames Processed:  21\n",
            "Frames Processed:  22\n",
            "Frames Processed:  23\n",
            "Frames Processed:  24\n",
            "Frames Processed:  25\n",
            "Frames Processed:  26\n",
            "Frames Processed:  27\n",
            "Frames Processed:  28\n",
            "Frames Processed:  29\n",
            "Frames Processed:  30\n",
            "Frames Processed:  31\n",
            "Frames Processed:  32\n",
            "Frames Processed:  33\n",
            "Frames Processed:  34\n",
            "Frames Processed:  35\n",
            "Frames Processed:  36\n",
            "Frames Processed:  37\n",
            "Frames Processed:  38\n",
            "Frames Processed:  39\n",
            "Frames Processed:  40\n",
            "Frames Processed:  41\n",
            "Frames Processed:  42\n",
            "Frames Processed:  43\n",
            "Frames Processed:  44\n",
            "Frames Processed:  45\n",
            "Frames Processed:  46\n",
            "Frames Processed:  47\n",
            "Frames Processed:  48\n",
            "Frames Processed:  49\n",
            "Frames Processed:  50\n",
            "Frames Processed:  51\n",
            "Frames Processed:  52\n",
            "Frames Processed:  53\n",
            "Frames Processed:  54\n",
            "Frames Processed:  55\n",
            "Frames Processed:  56\n",
            "Frames Processed:  57\n",
            "Frames Processed:  58\n",
            "Frames Processed:  59\n",
            "Frames Processed:  60\n",
            "Frames Processed:  61\n",
            "Frames Processed:  62\n",
            "Frames Processed:  63\n",
            "Frames Processed:  64\n",
            "Frames Processed:  65\n",
            "Frames Processed:  66\n",
            "Frames Processed:  67\n",
            "Frames Processed:  68\n",
            "Frames Processed:  69\n",
            "Frames Processed:  70\n",
            "Frames Processed:  71\n",
            "Frames Processed:  72\n",
            "Frames Processed:  73\n",
            "Frames Processed:  74\n",
            "Frames Processed:  75\n",
            "Frames Processed:  76\n",
            "Frames Processed:  77\n",
            "Frames Processed:  78\n",
            "Frames Processed:  79\n",
            "Frames Processed:  80\n",
            "Frames Processed:  81\n",
            "Frames Processed:  82\n",
            "Frames Processed:  83\n",
            "Frames Processed:  84\n",
            "Frames Processed:  85\n",
            "Frames Processed:  86\n",
            "Frames Processed:  87\n",
            "Frames Processed:  88\n",
            "Frames Processed:  89\n",
            "Frames Processed:  90\n",
            "Frames Processed:  91\n",
            "Frames Processed:  92\n",
            "Frames Processed:  93\n",
            "Frames Processed:  94\n",
            "Frames Processed:  95\n",
            "Frames Processed:  96\n",
            "Frames Processed:  97\n",
            "Frames Processed:  98\n",
            "Frames Processed:  99\n",
            "Frames Processed:  100\n",
            "Frames Processed:  101\n",
            "Frames Processed:  102\n",
            "Frames Processed:  103\n",
            "Frames Processed:  104\n",
            "Frames Processed:  105\n",
            "Frames Processed:  106\n",
            "Frames Processed:  107\n",
            "Frames Processed:  108\n",
            "Frames Processed:  109\n",
            "Frames Processed:  110\n",
            "Frames Processed:  111\n",
            "Frames Processed:  112\n",
            "Frames Processed:  113\n",
            "Frames Processed:  114\n",
            "Frames Processed:  115\n",
            "Frames Processed:  116\n",
            "Frames Processed:  117\n",
            "Frames Processed:  118\n",
            "Frames Processed:  119\n",
            "Frames Processed:  120\n",
            "Frames Processed:  121\n",
            "Frames Processed:  122\n",
            "Frames Processed:  123\n",
            "Frames Processed:  124\n",
            "Frames Processed:  125\n",
            "Frames Processed:  126\n",
            "Frames Processed:  127\n",
            "Frames Processed:  128\n",
            "Frames Processed:  129\n",
            "Frames Processed:  130\n",
            "Frames Processed:  131\n",
            "Frames Processed:  132\n",
            "Frames Processed:  133\n",
            "Frames Processed:  134\n",
            "Frames Processed:  135\n",
            "Frames Processed:  136\n",
            "Frames Processed:  137\n",
            "Frames Processed:  138\n",
            "Frames Processed:  139\n",
            "Frames Processed:  140\n",
            "Frames Processed:  141\n",
            "Frames Processed:  142\n",
            "Frames Processed:  143\n",
            "Frames Processed:  144\n",
            "Frames Processed:  145\n",
            "Frames Processed:  146\n",
            "Frames Processed:  147\n",
            "Frames Processed:  148\n",
            "Frames Processed:  149\n",
            "Frames Processed:  150\n",
            "Frames Processed:  151\n",
            "Frames Processed:  152\n",
            "Frames Processed:  153\n",
            "Frames Processed:  154\n",
            "Frames Processed:  155\n",
            "Frames Processed:  156\n",
            "Frames Processed:  157\n",
            "Frames Processed:  158\n",
            "Frames Processed:  159\n",
            "Frames Processed:  160\n",
            "Frames Processed:  161\n",
            "Frames Processed:  162\n",
            "Frames Processed:  163\n",
            "Frames Processed:  164\n",
            "Frames Processed:  165\n",
            "Frames Processed:  166\n",
            "Frames Processed:  167\n",
            "Frames Processed:  168\n",
            "Frames Processed:  169\n",
            "Frames Processed:  170\n",
            "Frames Processed:  171\n",
            "Frames Processed:  172\n",
            "Frames Processed:  173\n",
            "Frames Processed:  174\n",
            "Frames Processed:  175\n",
            "Frames Processed:  176\n",
            "Frames Processed:  177\n",
            "Frames Processed:  178\n",
            "Frames Processed:  179\n",
            "Frames Processed:  180\n",
            "Frames Processed:  181\n",
            "Frames Processed:  182\n",
            "Frames Processed:  183\n",
            "Frames Processed:  184\n",
            "Frames Processed:  185\n",
            "Frames Processed:  186\n",
            "Frames Processed:  187\n",
            "Frames Processed:  188\n",
            "Frames Processed:  189\n",
            "Frames Processed:  190\n",
            "Frames Processed:  191\n",
            "Frames Processed:  192\n",
            "Frames Processed:  193\n",
            "Frames Processed:  194\n",
            "Frames Processed:  195\n",
            "Frames Processed:  196\n",
            "Frames Processed:  197\n",
            "Frames Processed:  198\n",
            "Frames Processed:  199\n",
            "Frames Processed:  200\n",
            "Frames Processed:  201\n",
            "Frames Processed:  202\n",
            "Frames Processed:  203\n",
            "Frames Processed:  204\n",
            "Frames Processed:  205\n",
            "Frames Processed:  206\n",
            "Frames Processed:  207\n",
            "Frames Processed:  208\n",
            "Frames Processed:  209\n",
            "Frames Processed:  210\n",
            "Frames Processed:  211\n",
            "Frames Processed:  212\n",
            "Frames Processed:  213\n",
            "Frames Processed:  214\n",
            "Frames Processed:  215\n",
            "Frames Processed:  216\n",
            "Frames Processed:  217\n",
            "Frames Processed:  218\n",
            "Frames Processed:  219\n",
            "Frames Processed:  220\n",
            "Frames Processed:  221\n",
            "Frames Processed:  222\n",
            "Frames Processed:  223\n",
            "Frames Processed:  224\n",
            "Frames Processed:  225\n",
            "Frames Processed:  226\n",
            "Frames Processed:  227\n",
            "Frames Processed:  228\n",
            "Frames Processed:  229\n",
            "Frames Processed:  230\n",
            "Frames Processed:  231\n",
            "Frames Processed:  232\n",
            "Frames Processed:  233\n",
            "Frames Processed:  234\n",
            "Frames Processed:  235\n",
            "Frames Processed:  236\n",
            "Frames Processed:  237\n",
            "Frames Processed:  238\n",
            "Frames Processed:  239\n",
            "Frames Processed:  240\n",
            "Frames Processed:  241\n",
            "Frames Processed:  242\n",
            "Frames Processed:  243\n",
            "Frames Processed:  244\n",
            "Frames Processed:  245\n",
            "Frames Processed:  246\n",
            "Frames Processed:  247\n",
            "Frames Processed:  248\n",
            "Frames Processed:  249\n",
            "Frames Processed:  250\n",
            "Frames Processed:  251\n",
            "Frames Processed:  252\n",
            "Frames Processed:  253\n",
            "Frames Processed:  254\n",
            "Frames Processed:  255\n",
            "Frames Processed:  256\n",
            "Frames Processed:  257\n",
            "Frames Processed:  258\n",
            "Frames Processed:  259\n",
            "Frames Processed:  260\n",
            "Frames Processed:  261\n",
            "Frames Processed:  262\n",
            "Frames Processed:  263\n",
            "Frames Processed:  264\n",
            "Frames Processed:  265\n",
            "Frames Processed:  266\n",
            "Frames Processed:  267\n",
            "Frames Processed:  268\n",
            "Frames Processed:  269\n",
            "Frames Processed:  270\n",
            "Frames Processed:  271\n",
            "Frames Processed:  272\n",
            "Frames Processed:  273\n",
            "Frames Processed:  274\n",
            "Frames Processed:  275\n",
            "Frames Processed:  276\n",
            "Frames Processed:  277\n",
            "Frames Processed:  278\n",
            "Frames Processed:  279\n",
            "Frames Processed:  280\n",
            "Frames Processed:  281\n",
            "Frames Processed:  282\n",
            "Frames Processed:  283\n",
            "Frames Processed:  284\n",
            "Frames Processed:  285\n",
            "Frames Processed:  286\n",
            "Frames Processed:  287\n",
            "Frames Processed:  288\n",
            "Frames Processed:  289\n",
            "Frames Processed:  290\n",
            "Frames Processed:  291\n",
            "Frames Processed:  292\n",
            "Frames Processed:  293\n",
            "Frames Processed:  294\n",
            "Frames Processed:  295\n",
            "Frames Processed:  296\n",
            "Frames Processed:  297\n",
            "Frames Processed:  298\n",
            "Frames Processed:  299\n",
            "Frames Processed:  300\n",
            "Frames Processed:  301\n",
            "Frames Processed:  302\n",
            "Frames Processed:  303\n",
            "Frames Processed:  304\n",
            "Frames Processed:  305\n",
            "Frames Processed:  306\n",
            "Frames Processed:  307\n",
            "Frames Processed:  308\n",
            "Frames Processed:  309\n",
            "Frames Processed:  310\n",
            "Frames Processed:  311\n",
            "Frames Processed:  312\n",
            "Frames Processed:  313\n",
            "Frames Processed:  314\n",
            "Frames Processed:  315\n",
            "Frames Processed:  316\n",
            "Frames Processed:  317\n",
            "Frames Processed:  318\n",
            "Frames Processed:  319\n",
            "Frames Processed:  320\n",
            "Frames Processed:  321\n",
            "Frames Processed:  322\n",
            "Frames Processed:  323\n",
            "Frames Processed:  324\n",
            "Frames Processed:  325\n",
            "Frames Processed:  326\n",
            "Frames Processed:  327\n",
            "Frames Processed:  328\n",
            "Frames Processed:  329\n",
            "Frames Processed:  330\n",
            "Frames Processed:  331\n",
            "Frames Processed:  332\n",
            "Frames Processed:  333\n",
            "Frames Processed:  334\n",
            "Frames Processed:  335\n",
            "Frames Processed:  336\n",
            "Frames Processed:  337\n",
            "Frames Processed:  338\n",
            "Frames Processed:  339\n",
            "Frames Processed:  340\n",
            "Frames Processed:  341\n",
            "Frames Processed:  342\n",
            "Frames Processed:  343\n",
            "Frames Processed:  344\n",
            "Frames Processed:  345\n",
            "Frames Processed:  346\n",
            "Frames Processed:  347\n",
            "Frames Processed:  348\n",
            "Frames Processed:  349\n",
            "Frames Processed:  350\n",
            "Frames Processed:  351\n",
            "Frames Processed:  352\n",
            "Frames Processed:  353\n",
            "Frames Processed:  354\n",
            "Frames Processed:  355\n",
            "Frames Processed:  356\n",
            "Frames Processed:  357\n",
            "Frames Processed:  358\n",
            "Frames Processed:  359\n",
            "Frames Processed:  360\n",
            "Frames Processed:  361\n",
            "Frames Processed:  362\n",
            "Frames Processed:  363\n",
            "Frames Processed:  364\n",
            "Frames Processed:  365\n",
            "Frames Processed:  366\n",
            "Frames Processed:  367\n",
            "Frames Processed:  368\n",
            "Frames Processed:  369\n",
            "Frames Processed:  370\n",
            "Frames Processed:  371\n",
            "Frames Processed:  372\n",
            "Frames Processed:  373\n",
            "Frames Processed:  374\n",
            "Frames Processed:  375\n",
            "Frames Processed:  376\n",
            "Frames Processed:  377\n",
            "Frames Processed:  378\n",
            "Frames Processed:  379\n",
            "Frames Processed:  380\n",
            "Frames Processed:  381\n",
            "Frames Processed:  382\n",
            "Frames Processed:  383\n",
            "Frames Processed:  384\n",
            "Frames Processed:  385\n",
            "Frames Processed:  386\n",
            "Frames Processed:  387\n",
            "Frames Processed:  388\n",
            "Frames Processed:  389\n",
            "Frames Processed:  390\n",
            "Frames Processed:  391\n",
            "Frames Processed:  392\n",
            "Frames Processed:  393\n",
            "Frames Processed:  394\n",
            "Frames Processed:  395\n",
            "Frames Processed:  396\n",
            "Frames Processed:  397\n",
            "Frames Processed:  398\n",
            "Frames Processed:  399\n",
            "Frames Processed:  400\n",
            "Frames Processed:  401\n",
            "Frames Processed:  402\n",
            "Frames Processed:  403\n",
            "Frames Processed:  404\n",
            "Frames Processed:  405\n",
            "Frames Processed:  406\n",
            "Frames Processed:  407\n",
            "Frames Processed:  408\n",
            "Frames Processed:  409\n",
            "Frames Processed:  410\n",
            "Frames Processed:  411\n",
            "Frames Processed:  412\n",
            "Frames Processed:  413\n",
            "Frames Processed:  414\n",
            "Frames Processed:  415\n",
            "Frames Processed:  416\n",
            "Frames Processed:  417\n",
            "Frames Processed:  418\n",
            "Frames Processed:  419\n",
            "Frames Processed:  420\n",
            "Frames Processed:  421\n",
            "Frames Processed:  422\n",
            "Frames Processed:  423\n",
            "Frames Processed:  424\n",
            "Frames Processed:  425\n",
            "Frames Processed:  426\n",
            "Frames Processed:  427\n",
            "Frames Processed:  428\n",
            "Frames Processed:  429\n",
            "Frames Processed:  430\n",
            "Frames Processed:  431\n",
            "Frames Processed:  432\n",
            "Frames Processed:  433\n",
            "Frames Processed:  434\n",
            "Frames Processed:  435\n",
            "Frames Processed:  436\n",
            "Frames Processed:  437\n",
            "Frames Processed:  438\n",
            "Frames Processed:  439\n",
            "Frames Processed:  440\n",
            "Frames Processed:  441\n",
            "Frames Processed:  442\n",
            "Frames Processed:  443\n",
            "Frames Processed:  444\n",
            "Frames Processed:  445\n",
            "Frames Processed:  446\n",
            "Frames Processed:  447\n",
            "Frames Processed:  448\n",
            "Frames Processed:  449\n",
            "Frames Processed:  450\n",
            "Frames Processed:  451\n",
            "Frames Processed:  452\n",
            "Frames Processed:  453\n",
            "Frames Processed:  454\n",
            "Frames Processed:  455\n",
            "Frames Processed:  456\n",
            "Frames Processed:  457\n",
            "Frames Processed:  458\n",
            "Frames Processed:  459\n",
            "Frames Processed:  460\n",
            "Frames Processed:  461\n",
            "Frames Processed:  462\n",
            "Frames Processed:  463\n",
            "Frames Processed:  464\n",
            "Frames Processed:  465\n",
            "Frames Processed:  466\n",
            "Frames Processed:  467\n",
            "Frames Processed:  468\n",
            "Frames Processed:  469\n",
            "Frames Processed:  470\n",
            "Frames Processed:  471\n",
            "Frames Processed:  472\n",
            "Frames Processed:  473\n",
            "Frames Processed:  474\n",
            "Frames Processed:  475\n",
            "Frames Processed:  476\n",
            "Frames Processed:  477\n",
            "Frames Processed:  478\n",
            "Frames Processed:  479\n",
            "Frames Processed:  480\n",
            "Frames Processed:  481\n",
            "Frames Processed:  482\n",
            "Frames Processed:  483\n",
            "Frames Processed:  484\n",
            "Frames Processed:  485\n",
            "Frames Processed:  486\n",
            "Frames Processed:  487\n",
            "Frames Processed:  488\n",
            "Frames Processed:  489\n",
            "Frames Processed:  490\n",
            "Frames Processed:  491\n",
            "Frames Processed:  492\n",
            "Frames Processed:  493\n",
            "Frames Processed:  494\n",
            "Frames Processed:  495\n",
            "Frames Processed:  496\n",
            "Frames Processed:  497\n",
            "Frames Processed:  498\n",
            "Frames Processed:  499\n",
            "Frames Processed:  500\n",
            "Frames Processed:  501\n",
            "Frames Processed:  502\n",
            "Frames Processed:  503\n",
            "Frames Processed:  504\n",
            "Frames Processed:  505\n",
            "Frames Processed:  506\n",
            "Frames Processed:  507\n",
            "Frames Processed:  508\n",
            "Frames Processed:  509\n",
            "Frames Processed:  510\n",
            "Frames Processed:  511\n",
            "Frames Processed:  512\n",
            "Frames Processed:  513\n",
            "Frames Processed:  514\n",
            "Frames Processed:  515\n",
            "Frames Processed:  516\n",
            "Frames Processed:  517\n",
            "Frames Processed:  518\n",
            "Frames Processed:  519\n",
            "Frames Processed:  520\n",
            "Frames Processed:  521\n",
            "Frames Processed:  522\n",
            "Frames Processed:  523\n",
            "Frames Processed:  524\n",
            "Frames Processed:  525\n",
            "Frames Processed:  526\n",
            "Frames Processed:  527\n",
            "Frames Processed:  528\n",
            "Frames Processed:  529\n",
            "Frames Processed:  530\n",
            "Frames Processed:  531\n",
            "Frames Processed:  532\n",
            "Frames Processed:  533\n",
            "Frames Processed:  534\n",
            "Frames Processed:  535\n",
            "Frames Processed:  536\n",
            "Frames Processed:  537\n",
            "Frames Processed:  538\n",
            "Frames Processed:  539\n",
            "Frames Processed:  540\n",
            "Frames Processed:  541\n",
            "Frames Processed:  542\n",
            "Frames Processed:  543\n",
            "Frames Processed:  544\n",
            "Frames Processed:  545\n",
            "Frames Processed:  546\n",
            "Frames Processed:  547\n",
            "Frames Processed:  548\n",
            "Frames Processed:  549\n",
            "Frames Processed:  550\n",
            "Frames Processed:  551\n",
            "Frames Processed:  552\n",
            "Frames Processed:  553\n",
            "Frames Processed:  554\n",
            "Frames Processed:  555\n",
            "Frames Processed:  556\n",
            "Frames Processed:  557\n",
            "Frames Processed:  558\n",
            "Frames Processed:  559\n",
            "Frames Processed:  560\n",
            "Frames Processed:  561\n",
            "Frames Processed:  562\n",
            "Frames Processed:  563\n",
            "Frames Processed:  564\n",
            "Frames Processed:  565\n",
            "Frames Processed:  566\n",
            "Frames Processed:  567\n",
            "Frames Processed:  568\n",
            "Frames Processed:  569\n",
            "Frames Processed:  570\n",
            "Frames Processed:  571\n",
            "Frames Processed:  572\n",
            "Frames Processed:  573\n",
            "Frames Processed:  574\n",
            "Frames Processed:  575\n",
            "Frames Processed:  576\n",
            "Frames Processed:  577\n",
            "Frames Processed:  578\n",
            "Frames Processed:  579\n",
            "Frames Processed:  580\n",
            "Frames Processed:  581\n",
            "Frames Processed:  582\n",
            "Frames Processed:  583\n",
            "Frames Processed:  584\n",
            "Frames Processed:  585\n",
            "Frames Processed:  586\n",
            "Frames Processed:  587\n",
            "Frames Processed:  588\n",
            "Frames Processed:  589\n",
            "Frames Processed:  590\n",
            "Frames Processed:  591\n",
            "Frames Processed:  592\n",
            "Frames Processed:  593\n",
            "Frames Processed:  594\n",
            "Frames Processed:  595\n",
            "Frames Processed:  596\n",
            "Frames Processed:  597\n",
            "Frames Processed:  598\n",
            "Frames Processed:  599\n",
            "Frames Processed:  600\n",
            "Frames Processed:  601\n",
            "Frames Processed:  602\n",
            "Frames Processed:  603\n",
            "Frames Processed:  604\n",
            "Frames Processed:  605\n",
            "Frames Processed:  606\n",
            "Frames Processed:  607\n",
            "Frames Processed:  608\n",
            "Frames Processed:  609\n",
            "Frames Processed:  610\n",
            "Frames Processed:  611\n",
            "Frames Processed:  612\n",
            "Frames Processed:  613\n",
            "Frames Processed:  614\n",
            "Frames Processed:  615\n",
            "Frames Processed:  616\n",
            "Frames Processed:  617\n",
            "Frames Processed:  618\n",
            "Frames Processed:  619\n",
            "Frames Processed:  620\n",
            "Frames Processed:  621\n",
            "Frames Processed:  622\n",
            "Frames Processed:  623\n",
            "Frames Processed:  624\n",
            "Frames Processed:  625\n",
            "Frames Processed:  626\n",
            "Frames Processed:  627\n",
            "Frames Processed:  628\n",
            "Frames Processed:  629\n",
            "Frames Processed:  630\n",
            "Frames Processed:  631\n",
            "Frames Processed:  632\n",
            "Frames Processed:  633\n",
            "Frames Processed:  634\n",
            "Frames Processed:  635\n",
            "Frames Processed:  636\n",
            "Frames Processed:  637\n",
            "Frames Processed:  638\n",
            "Frames Processed:  639\n",
            "Frames Processed:  640\n",
            "Frames Processed:  641\n",
            "Frames Processed:  642\n",
            "Frames Processed:  643\n",
            "Frames Processed:  644\n",
            "Frames Processed:  645\n",
            "Frames Processed:  646\n",
            "Frames Processed:  647\n",
            "Frames Processed:  648\n",
            "Frames Processed:  649\n",
            "Frames Processed:  650\n",
            "Frames Processed:  651\n",
            "Frames Processed:  652\n",
            "Frames Processed:  653\n",
            "Frames Processed:  654\n",
            "Frames Processed:  655\n",
            "Frames Processed:  656\n",
            "Frames Processed:  657\n",
            "Frames Processed:  658\n",
            "Frames Processed:  659\n",
            "Frames Processed:  660\n",
            "Frames Processed:  661\n",
            "Frames Processed:  662\n",
            "Frames Processed:  663\n",
            "Frames Processed:  664\n",
            "Frames Processed:  665\n",
            "Frames Processed:  666\n",
            "Frames Processed:  667\n",
            "Frames Processed:  668\n",
            "Frames Processed:  669\n",
            "Frames Processed:  670\n",
            "Frames Processed:  671\n",
            "Frames Processed:  672\n",
            "Frames Processed:  673\n",
            "Frames Processed:  674\n",
            "Frames Processed:  675\n",
            "Frames Processed:  676\n",
            "Frames Processed:  677\n",
            "Frames Processed:  678\n",
            "Frames Processed:  679\n",
            "Frames Processed:  680\n",
            "Frames Processed:  681\n",
            "Frames Processed:  682\n",
            "Frames Processed:  683\n",
            "Frames Processed:  684\n",
            "Frames Processed:  685\n",
            "Frames Processed:  686\n",
            "Frames Processed:  687\n",
            "Frames Processed:  688\n",
            "Frames Processed:  689\n",
            "Frames Processed:  690\n",
            "Frames Processed:  691\n",
            "Frames Processed:  692\n",
            "Frames Processed:  693\n",
            "Frames Processed:  694\n",
            "Frames Processed:  695\n",
            "Frames Processed:  696\n",
            "Frames Processed:  697\n",
            "Frames Processed:  698\n",
            "Frames Processed:  699\n",
            "Frames Processed:  700\n",
            "Frames Processed:  701\n",
            "Frames Processed:  702\n",
            "Frames Processed:  703\n",
            "Frames Processed:  704\n",
            "Frames Processed:  705\n",
            "Frames Processed:  706\n",
            "Frames Processed:  707\n",
            "Frames Processed:  708\n",
            "Frames Processed:  709\n",
            "Frames Processed:  710\n",
            "Frames Processed:  711\n",
            "Frames Processed:  712\n",
            "Frames Processed:  713\n",
            "Frames Processed:  714\n",
            "Frames Processed:  715\n",
            "Frames Processed:  716\n",
            "Frames Processed:  717\n",
            "Frames Processed:  718\n",
            "Frames Processed:  719\n",
            "Frames Processed:  720\n",
            "Frames Processed:  721\n",
            "Frames Processed:  722\n",
            "Frames Processed:  723\n",
            "Frames Processed:  724\n",
            "Frames Processed:  725\n",
            "Frames Processed:  726\n",
            "Frames Processed:  727\n",
            "Frames Processed:  728\n",
            "Frames Processed:  729\n",
            "Frames Processed:  730\n",
            "Frames Processed:  731\n",
            "Frames Processed:  732\n",
            "Frames Processed:  733\n",
            "Frames Processed:  734\n",
            "Frames Processed:  735\n",
            "Frames Processed:  736\n",
            "Frames Processed:  737\n",
            "Frames Processed:  738\n",
            "Frames Processed:  739\n",
            "Frames Processed:  740\n",
            "Frames Processed:  741\n",
            "Frames Processed:  742\n",
            "Frames Processed:  743\n",
            "Frames Processed:  744\n",
            "Frames Processed:  745\n",
            "Frames Processed:  746\n",
            "Frames Processed:  747\n",
            "Frames Processed:  748\n",
            "Frames Processed:  749\n",
            "Frames Processed:  750\n",
            "Frames Processed:  751\n",
            "Frames Processed:  752\n",
            "Frames Processed:  753\n",
            "Frames Processed:  754\n",
            "Frames Processed:  755\n",
            "Frames Processed:  756\n",
            "Frames Processed:  757\n",
            "Frames Processed:  758\n",
            "Frames Processed:  759\n",
            "Frames Processed:  760\n",
            "Frames Processed:  761\n",
            "Frames Processed:  762\n",
            "Frames Processed:  763\n",
            "Frames Processed:  764\n",
            "Frames Processed:  765\n",
            "Frames Processed:  766\n",
            "Frames Processed:  767\n",
            "Frames Processed:  768\n",
            "Frames Processed:  769\n",
            "Frames Processed:  770\n",
            "Frames Processed:  771\n",
            "Frames Processed:  772\n",
            "Frames Processed:  773\n",
            "Frames Processed:  774\n",
            "Frames Processed:  775\n",
            "Frames Processed:  776\n",
            "Frames Processed:  777\n",
            "Frames Processed:  778\n",
            "Frames Processed:  779\n",
            "Frames Processed:  780\n",
            "Frames Processed:  781\n",
            "Frames Processed:  782\n",
            "Frames Processed:  783\n",
            "Frames Processed:  784\n",
            "Frames Processed:  785\n",
            "Frames Processed:  786\n",
            "Frames Processed:  787\n",
            "Frames Processed:  788\n",
            "Frames Processed:  789\n",
            "Frames Processed:  790\n",
            "Frames Processed:  791\n",
            "Frames Processed:  792\n",
            "Frames Processed:  793\n",
            "Frames Processed:  794\n",
            "Frames Processed:  795\n",
            "Frames Processed:  796\n",
            "Frames Processed:  797\n",
            "Frames Processed:  798\n",
            "Frames Processed:  799\n",
            "Frames Processed:  800\n",
            "Frames Processed:  801\n",
            "Frames Processed:  802\n",
            "Frames Processed:  803\n",
            "Frames Processed:  804\n",
            "Frames Processed:  805\n",
            "Frames Processed:  806\n",
            "Frames Processed:  807\n",
            "Frames Processed:  808\n",
            "Frames Processed:  809\n",
            "Frames Processed:  810\n",
            "Frames Processed:  811\n",
            "Frames Processed:  812\n",
            "Frames Processed:  813\n",
            "Frames Processed:  814\n",
            "Frames Processed:  815\n",
            "Frames Processed:  816\n",
            "Frames Processed:  817\n",
            "Frames Processed:  818\n",
            "Frames Processed:  819\n",
            "Frames Processed:  820\n",
            "Frames Processed:  821\n",
            "Frames Processed:  822\n",
            "Frames Processed:  823\n",
            "Frames Processed:  824\n",
            "Frames Processed:  825\n",
            "Frames Processed:  826\n",
            "Frames Processed:  827\n",
            "Frames Processed:  828\n",
            "Frames Processed:  829\n",
            "Frames Processed:  830\n",
            "Frames Processed:  831\n",
            "Frames Processed:  832\n",
            "Frames Processed:  833\n",
            "Frames Processed:  834\n",
            "Frames Processed:  835\n",
            "Frames Processed:  836\n",
            "Frames Processed:  837\n",
            "Frames Processed:  838\n",
            "Frames Processed:  839\n",
            "Frames Processed:  840\n",
            "Frames Processed:  841\n",
            "Frames Processed:  842\n",
            "Frames Processed:  843\n",
            "Frames Processed:  844\n",
            "Frames Processed:  845\n",
            "Frames Processed:  846\n",
            "Frames Processed:  847\n",
            "Frames Processed:  848\n",
            "Frames Processed:  849\n",
            "Frames Processed:  850\n",
            "Frames Processed:  851\n",
            "Frames Processed:  852\n",
            "Frames Processed:  853\n",
            "Frames Processed:  854\n",
            "Frames Processed:  855\n",
            "Frames Processed:  856\n",
            "Frames Processed:  857\n",
            "Frames Processed:  858\n",
            "Frames Processed:  859\n",
            "Frames Processed:  860\n",
            "Frames Processed:  861\n",
            "Frames Processed:  862\n",
            "Frames Processed:  863\n",
            "Frames Processed:  864\n",
            "Frames Processed:  865\n",
            "Frames Processed:  866\n",
            "Frames Processed:  867\n",
            "Frames Processed:  868\n",
            "Frames Processed:  869\n",
            "Frames Processed:  870\n",
            "Frames Processed:  871\n",
            "Frames Processed:  872\n",
            "Frames Processed:  873\n",
            "Frames Processed:  874\n",
            "Frames Processed:  875\n",
            "Frames Processed:  876\n",
            "Frames Processed:  877\n",
            "Frames Processed:  878\n",
            "Frames Processed:  879\n",
            "Frames Processed:  880\n",
            "Frames Processed:  881\n",
            "Frames Processed:  882\n",
            "Frames Processed:  883\n",
            "Frames Processed:  884\n",
            "Frames Processed:  885\n",
            "Frames Processed:  886\n",
            "Frames Processed:  887\n",
            "Frames Processed:  888\n",
            "Frames Processed:  889\n",
            "Frames Processed:  890\n",
            "Frames Processed:  891\n",
            "Frames Processed:  892\n",
            "Frames Processed:  893\n",
            "Frames Processed:  894\n",
            "Frames Processed:  895\n",
            "Frames Processed:  896\n",
            "Frames Processed:  897\n",
            "Frames Processed:  898\n",
            "Frames Processed:  899\n",
            "Frames Processed:  900\n",
            "Frames Processed:  901\n",
            "Frames Processed:  902\n",
            "Frames Processed:  903\n",
            "Frames Processed:  904\n",
            "Frames Processed:  905\n",
            "Frames Processed:  906\n",
            "Frames Processed:  907\n",
            "Frames Processed:  908\n",
            "Frames Processed:  909\n",
            "Frames Processed:  910\n",
            "Frames Processed:  911\n",
            "Frames Processed:  912\n",
            "Frames Processed:  913\n",
            "Frames Processed:  914\n",
            "Frames Processed:  915\n",
            "Frames Processed:  916\n",
            "Frames Processed:  917\n",
            "Frames Processed:  918\n",
            "Frames Processed:  919\n",
            "Frames Processed:  920\n",
            "Frames Processed:  921\n",
            "Frames Processed:  922\n",
            "Frames Processed:  923\n",
            "Frames Processed:  924\n",
            "Frames Processed:  925\n",
            "Frames Processed:  926\n",
            "Frames Processed:  927\n",
            "Frames Processed:  928\n",
            "Frames Processed:  929\n",
            "Frames Processed:  930\n",
            "Frames Processed:  931\n",
            "Frames Processed:  932\n",
            "Frames Processed:  933\n",
            "Frames Processed:  934\n",
            "Frames Processed:  935\n",
            "Frames Processed:  936\n",
            "Frames Processed:  937\n",
            "Frames Processed:  938\n",
            "Frames Processed:  939\n",
            "Frames Processed:  940\n",
            "Frames Processed:  941\n",
            "Frames Processed:  942\n",
            "Frames Processed:  943\n",
            "Frames Processed:  944\n",
            "Frames Processed:  945\n",
            "Frames Processed:  946\n",
            "Frames Processed:  947\n",
            "Frames Processed:  948\n",
            "Frames Processed:  949\n",
            "Frames Processed:  950\n",
            "Frames Processed:  951\n",
            "Frames Processed:  952\n",
            "Frames Processed:  953\n",
            "Frames Processed:  954\n",
            "Frames Processed:  955\n",
            "Frames Processed:  956\n",
            "Frames Processed:  957\n",
            "Frames Processed:  958\n",
            "Frames Processed:  959\n",
            "Frames Processed:  960\n",
            "Frames Processed:  961\n",
            "Frames Processed:  962\n",
            "Frames Processed:  963\n",
            "Frames Processed:  964\n",
            "Frames Processed:  965\n",
            "Frames Processed:  966\n",
            "Frames Processed:  967\n",
            "Frames Processed:  968\n",
            "Frames Processed:  969\n",
            "Frames Processed:  970\n",
            "Frames Processed:  971\n",
            "Frames Processed:  972\n",
            "Frames Processed:  973\n",
            "Frames Processed:  974\n",
            "Frames Processed:  975\n",
            "Frames Processed:  976\n",
            "Frames Processed:  977\n",
            "Frames Processed:  978\n",
            "Frames Processed:  979\n",
            "Frames Processed:  980\n",
            "Frames Processed:  981\n",
            "Frames Processed:  982\n",
            "Frames Processed:  983\n",
            "Frames Processed:  984\n",
            "Frames Processed:  985\n",
            "Frames Processed:  986\n",
            "Frames Processed:  987\n",
            "Frames Processed:  988\n",
            "Frames Processed:  989\n",
            "Frames Processed:  990\n",
            "Frames Processed:  991\n",
            "Frames Processed:  992\n",
            "Frames Processed:  993\n",
            "Frames Processed:  994\n",
            "Frames Processed:  995\n",
            "Frames Processed:  996\n",
            "Frames Processed:  997\n",
            "Frames Processed:  998\n",
            "Frames Processed:  999\n",
            "Frames Processed:  1000\n",
            "Frames Processed:  1001\n",
            "Frames Processed:  1002\n",
            "Frames Processed:  1003\n",
            "Frames Processed:  1004\n",
            "Frames Processed:  1005\n",
            "Frames Processed:  1006\n",
            "Frames Processed:  1007\n",
            "Frames Processed:  1008\n",
            "Frames Processed:  1009\n",
            "Frames Processed:  1010\n",
            "Frames Processed:  1011\n",
            "Frames Processed:  1012\n",
            "Frames Processed:  1013\n",
            "Frames Processed:  1014\n",
            "Frames Processed:  1015\n",
            "Frames Processed:  1016\n",
            "Frames Processed:  1017\n",
            "Frames Processed:  1018\n",
            "Frames Processed:  1019\n",
            "Frames Processed:  1020\n",
            "Frames Processed:  1021\n",
            "Frames Processed:  1022\n",
            "Frames Processed:  1023\n",
            "Frames Processed:  1024\n",
            "Frames Processed:  1025\n",
            "Frames Processed:  1026\n",
            "Frames Processed:  1027\n",
            "Frames Processed:  1028\n",
            "Frames Processed:  1029\n",
            "Frames Processed:  1030\n",
            "Frames Processed:  1031\n",
            "Frames Processed:  1032\n",
            "Frames Processed:  1033\n",
            "Frames Processed:  1034\n",
            "Frames Processed:  1035\n",
            "Frames Processed:  1036\n",
            "Frames Processed:  1037\n",
            "Frames Processed:  1038\n",
            "Frames Processed:  1039\n",
            "Frames Processed:  1040\n",
            "Frames Processed:  1041\n",
            "Frames Processed:  1042\n",
            "Frames Processed:  1043\n",
            "Frames Processed:  1044\n",
            "Frames Processed:  1045\n",
            "Frames Processed:  1046\n",
            "Frames Processed:  1047\n",
            "Frames Processed:  1048\n",
            "Frames Processed:  1049\n",
            "Frames Processed:  1050\n",
            "Frames Processed:  1051\n",
            "Frames Processed:  1052\n",
            "Frames Processed:  1053\n",
            "Frames Processed:  1054\n",
            "Frames Processed:  1055\n",
            "Frames Processed:  1056\n",
            "Frames Processed:  1057\n",
            "Frames Processed:  1058\n",
            "Frames Processed:  1059\n",
            "Frames Processed:  1060\n",
            "Frames Processed:  1061\n",
            "Frames Processed:  1062\n",
            "Frames Processed:  1063\n",
            "Frames Processed:  1064\n",
            "Frames Processed:  1065\n",
            "Frames Processed:  1066\n",
            "Frames Processed:  1067\n",
            "Frames Processed:  1068\n",
            "Frames Processed:  1069\n",
            "Frames Processed:  1070\n",
            "Frames Processed:  1071\n",
            "Frames Processed:  1072\n",
            "Frames Processed:  1073\n",
            "Frames Processed:  1074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "qHWBcj0vwYnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep sort does have a few limitations. Deep sort uses appearance features to associate detections with a certain track and this can lead to being unable to differentiate very similar objects. It's even possible that a small lighting difference can cause a previously tracked player to be given a new identity."
      ],
      "metadata": {
        "id": "YAAu3ij9wadu"
      }
    }
  ]
}